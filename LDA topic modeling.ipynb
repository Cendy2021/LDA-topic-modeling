import nltk

file_content = open("data source.csv").read()

#Tokenize
tokens = nltk.word_tokenize(file_content)

#Lemmatization
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatized_output = [lemmatizer.lemmatize(w) for w in tokens]

# Stopwords
stopWords = []
with open('stopwords source.txt', "r") as f:
    for line in f.readlines():
        if line != "":
            stopWords.append(line.strip())

# Use sklearn import LDA
from sklearn.feature_extraction.text import CountVectorizer

count_para_vectorizer = CountVectorizer(stop_words=stopWords, min_df=5, max_df=0.7)
count_para_vectors = count_para_vectorizer.fit_transform(lemmatized_output)

from sklearn.decomposition import LatentDirichletAllocation

lda_para_model = LatentDirichletAllocation(n_components=10, random_state=42)
W_lda_para_matrix = lda_para_model.fit_transform(count_para_vectors)
H_lda_para_matrix = lda_para_model.components_

#Display topics
def display_topics(model, features, no_top_words=10):
    for topic, word_vector in enumerate(model.components_):
        total = word_vector.sum()
        largest = word_vector.argsort()[::-1]  # invert sort order
        print("\nTopic %02d" % topic)
        for i in range(0, no_top_words):
            print(" %s (%2.2f)" % (features[largest[i]],
                                   word_vector[largest[i]] * 100.0 / total))


display_topics(lda_para_model, count_para_vectorizer.get_feature_names())
